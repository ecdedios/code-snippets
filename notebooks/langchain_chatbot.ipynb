{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1ef11a4-20fa-4629-8c4b-053469c37b40",
   "metadata": {},
   "source": [
    "# BUILD A CHATBOT USING LANGCHAIN\n",
    "\n",
    "https://python.langchain.com/docs/tutorials/chatbot/\n",
    "\n",
    "## LangChain and LangGraph installation\n",
    "\n",
    "```\n",
    "pip install langchain-core langgraph>0.2.27\n",
    "```\n",
    "\n",
    "## LangSmith for tracing\n",
    "\n",
    "```\n",
    "LANGCHAIN_TRACING_V2=true  \n",
    "LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"  \n",
    "LANGCHAIN_API_KEY=\"xxx\"  \n",
    "LANGCHAIN_PROJECT=\"xxx\"\n",
    "```\n",
    "\n",
    "## LLM installation\n",
    "\n",
    "```\n",
    "pip install -qU langchain-openai                  #openai\n",
    "pip install -qU langchain-anthropic               #anthropic\n",
    "pip install -qU langchain-google-vertexai         #google\n",
    "pip install -qU langchain-groq                    #groq\n",
    "```\n",
    "\n",
    "# QUICKSTART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e38c7e2-904a-4fa2-9940-9261ed650be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Dd! How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 38, 'total_tokens': 51, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e2bde53e6e', 'finish_reason': 'stop', 'logprobs': None}, id='run-d85d4387-7dff-4760-ab2a-42ffed47efa2-0', usage_metadata={'input_tokens': 38, 'output_tokens': 13, 'total_tokens': 51, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! My name is Dd.\"),\n",
    "        AIMessage(content=\"Hello Dd! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9e62f0-5bf1-45d4-bd5e-0f143734df7a",
   "metadata": {},
   "source": [
    "# MESSAGE PERSISTENCE IN LANGGRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1652a674-859e-45cc-b635-2f965d97ff2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Dd! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# Config into runnable\n",
    "config = {\"configurable\": {\"thread_id\": \"dd001\"}}\n",
    "\n",
    "# Invoke the application\n",
    "\n",
    "query = \"Hi! My name is Dd.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b063074d-6307-44af-b6ff-a97b727df131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Dd! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cece6980-fcea-47d6-a287-94965927ca78",
   "metadata": {},
   "source": [
    "# PROMPT TEMPLATE - SIMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "065add31-8e41-4ae6-b880-e7cdefd4024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You talk like a pirate. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d807bc9-da98-4924-b93f-c339869e048a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Ahoy, Dd! Welcome aboard me ship! What be it ye be seekinâ€™ today? Arrr!\n"
     ]
    }
   ],
   "source": [
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    chain = prompt | model                                               # The new chain\n",
    "    response = chain.invoke(state)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# Config into runnable\n",
    "config = {\"configurable\": {\"thread_id\": \"dd002\"}}\n",
    "\n",
    "# Invoke the application\n",
    "\n",
    "query = \"Hi! My name is Dd.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b7ce90-9fec-4c78-b1ba-a9c09fd17487",
   "metadata": {},
   "source": [
    "# PROMPT TEMPLATE - PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5324ade2-9902-48bd-92ba-6ad1437f08e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a5fa544-8569-4e12-81c8-d6764be7040e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Kamusta, Dd! Ano ang maitutulong ko sa iyo ngayon?\n"
     ]
    }
   ],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "class State(TypedDict):                                                       # The new typeddict\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "def call_model(state: State):\n",
    "    chain = prompt | model\n",
    "    response = chain.invoke(state)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"dd003\"}}\n",
    "query = \"Hi! My name is Dd.\"\n",
    "language = \"Tagalog\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},                    # Includes the parameters\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7166dd77-c6a0-4900-9ad8-8ff1628f9877",
   "metadata": {},
   "source": [
    "# MANAGING CONVERSATION HISTORY\n",
    "\n",
    "- trimming - https://python.langchain.com/docs/how_to/trim_messages/\n",
    "- filtering - https://python.langchain.com/docs/how_to/filter_messages/\n",
    "- merging - https://python.langchain.com/docs/how_to/merge_message_runs/\n",
    "\n",
    "## Trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42173a08-2fcf-439b-a18e-a72e56889e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 x 3', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! My name is Dd.\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like rocky road ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 x 3\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdc266bd-dcfc-49f7-a291-1b5d19696b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "def call_model(state: State):\n",
    "    chain = prompt | model\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])                          # Trim the messages\n",
    "    response = chain.invoke(\n",
    "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
    "    )\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5761255-a756-4495-89b7-138f0ab6eb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Tinanong mo kung ano ang 2 x 3.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"dd004\"}}\n",
    "query = \"What math problem did I ask?\"\n",
    "language = \"Tagalog\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "960a7290-7405-4e6f-b457-363d149a8841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Ang sagot sa 2 x 3 ay 6.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"dd004\"}}\n",
    "query = \"Ano ang sagot?\"\n",
    "language = \"Tagalog\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051a67b8-3781-4841-b156-84b986f921e1",
   "metadata": {},
   "source": [
    "# Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4a859ca-3d27-4164-9b49-a8a19a5fdedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    filter_messages,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"you are a good assistant\", id=\"1\"),\n",
    "    HumanMessage(\"example input\", id=\"2\", name=\"example_user\"),\n",
    "    AIMessage(\"example output\", id=\"3\", name=\"example_assistant\"),\n",
    "    HumanMessage(\"real input\", id=\"4\", name=\"dd\"),\n",
    "    AIMessage(\"real output\", id=\"5\", name=\"albert\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a5c0cd3-d99f-4b5a-a510-8b47bdf11310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='example input', additional_kwargs={}, response_metadata={}, name='example_user', id='2'),\n",
       " HumanMessage(content='real input', additional_kwargs={}, response_metadata={}, name='dd', id='4')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_messages(messages, include_types=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6e21975-4c47-4c57-b9fc-7415c6fd2126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='you are a good assistant', additional_kwargs={}, response_metadata={}, id='1'),\n",
       " HumanMessage(content='real input', additional_kwargs={}, response_metadata={}, name='dd', id='4'),\n",
       " AIMessage(content='real output', additional_kwargs={}, response_metadata={}, name='albert', id='5')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_messages(messages, exclude_names=[\"example_user\", \"example_assistant\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af523395-62f1-4446-8727-58bbdfdd2360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='example input', additional_kwargs={}, response_metadata={}, name='example_user', id='2'),\n",
       " HumanMessage(content='real input', additional_kwargs={}, response_metadata={}, name='dd', id='4'),\n",
       " AIMessage(content='real output', additional_kwargs={}, response_metadata={}, name='albert', id='5')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_messages(messages, include_types=[HumanMessage, AIMessage], exclude_ids=[\"3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da761a4f-de0f-4564-bd90-163396ae9d4d",
   "metadata": {},
   "source": [
    "## Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d09c08f9-f346-4ff6-bd75-eefecddf22c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SystemMessage(content=\"you're a good assistant.\\nyou always respond with a joke.\", additional_kwargs={}, response_metadata={})\n",
      "\n",
      "HumanMessage(content=[{'type': 'text', 'text': \"i wonder why it's called langchain\"}, 'and who is harrison chasing anyways'], additional_kwargs={}, response_metadata={})\n",
      "\n",
      "AIMessage(content='Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!\\nWhy, he\\'s probably chasing after the last cup of coffee in the office!', additional_kwargs={}, response_metadata={})\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    merge_message_runs,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"you're a good assistant.\"),\n",
    "    SystemMessage(\"you always respond with a joke.\"),\n",
    "    HumanMessage([{\"type\": \"text\", \"text\": \"i wonder why it's called langchain\"}]),\n",
    "    HumanMessage(\"and who is harrison chasing anyways\"),\n",
    "    AIMessage(\n",
    "        'Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'\n",
    "    ),\n",
    "    AIMessage(\"Why, he's probably chasing after the last cup of coffee in the office!\"),\n",
    "]\n",
    "\n",
    "merged = merge_message_runs(messages)                                        # Merge messages\n",
    "print(\"\\n\\n\".join([repr(x) for x in merged]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b03bace-d342-4334-bdaa-db1970e6c606",
   "metadata": {},
   "source": [
    "# STREAMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e242758d-aa2e-4eb8-8737-03dd53780d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|K|um|usta|,| D|d|!| Nar|ito| ang| isang| biro| para| sa| iyo|:\n",
      "\n",
      "|B|akit| hindi| nag|tag|ump|ay| ang| skeleton| sa| pags|ali| sa| mga| party|?\n",
      "\n",
      "|D|ahil| wala| siyang| \"|g|uts|\"!| ðŸ˜„||"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"dd005\"}}\n",
    "query = \"Hi My name is Dd, please tell me a joke.\"\n",
    "language = \"Tagalog\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4c6e35-3e76-431a-a792-4139aaada9f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

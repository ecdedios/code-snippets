{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# setting the random seed for reproducibility\n",
    "import random\n",
    "random.seed(493)\n",
    "\n",
    "# for manipulating dataframes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for statistical testing\n",
    "from scipy import stats\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# natural language processing\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# for natural language processing\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# for modeling\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn import metrics\n",
    "\n",
    "# for time-series forecasting\n",
    "import matplotlib.pyplot as plt\n",
    "from prophet import Prophet\n",
    "\n",
    "# for working with timestamps\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "\n",
    "# for visualizations\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# to print out all the outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a csv file\n",
    "df = pd.read_csv('../data/in/short_survey_data_with_asat_rr_csat.csv')\n",
    "\n",
    "# Read an excel file\n",
    "df = pd.read_excel('../data/in/short_survey_data_with_asat_rr_csat.xlsx')\n",
    "\n",
    "# Read a tsv file\n",
    "df = pd.read_csv('../data/in/short_survey_data_with_asat_rr_csat.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading All Files Within a Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Read filenames from the given path\n",
    "data_files = os.listdir('path/to/datafiles')\n",
    "\n",
    "def load_files(filenames):\n",
    "    for filename in filenames:\n",
    "        yield pd.read_csv(filename)\n",
    "\n",
    "data = pd.concat(load_files(data_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_missing(df):\n",
    "    \"\"\"\n",
    "    Takes a dataframe and returns a dataframe with stats\n",
    "    on missing and null values with their percentages.\n",
    "    \"\"\"\n",
    "    null_count = df.isnull().sum()\n",
    "    null_percentage = (null_count / df.shape[0]) * 100\n",
    "    empty_count = pd.Series(((df == ' ') | (df == '')).sum())\n",
    "    empty_percentage = (empty_count / df.shape[0]) * 100\n",
    "    nan_count = pd.Series(((df == 'nan') | (df == 'NaN')).sum())\n",
    "    nan_percentage = (nan_count / df.shape[0]) * 100\n",
    "    dfx = pd.DataFrame({'num_missing': null_count, 'missing_percentage': null_percentage,\n",
    "                         'num_empty': empty_count, 'empty_percentage': empty_percentage,\n",
    "                         'nan_count': nan_count, 'nan_percentage': nan_percentage})\n",
    "    return dfx\n",
    "\n",
    "show_missing(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Value Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_values(df, columns):\n",
    "    \"\"\"\n",
    "    Take a dataframe and a list of columns and\n",
    "    returns the value counts for the columns.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        print(column)\n",
    "        print('=====================================')\n",
    "        print(df[column].value_counts(dropna=False))\n",
    "        print('\\n')\n",
    "\n",
    "def show_values(df, param):\n",
    "    if param == 'all':\n",
    "        get_values(df, df.columns)\n",
    "    else:\n",
    "        get_values(df, param) \n",
    "\n",
    "show_values(df, ['asat'])\n",
    "show_values(df, 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADDITIONAL_STOPWORDS = ['ignore', 'me']\n",
    "\n",
    "def clean(sentence):\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords = nltk.corpus.stopwords.words('english') + ADDITIONAL_STOPWORDS\n",
    "    sentence = (unicodedata.normalize('NFKD', sentence)\n",
    "        .encode('ascii', 'ignore')\n",
    "        .decode('utf-8', 'ignore')\n",
    "        .lower())\n",
    "    words = re.sub(r'[^\\w\\s]', '', sentence).split()\n",
    "    word_list = [wnl.lemmatize(word) for word in words if word not in stopwords]\n",
    "    return word_list\n",
    "\n",
    "clean('The quick brown fox jumps over the lazy dog. Ignore me.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_words(df, column):\n",
    "    \"\"\"\n",
    "    Takes a dataframe and a column and returns a list of\n",
    "    cleaned words that is returned by clean().\n",
    "\n",
    "            Parameters:\n",
    "                    df (dataframe): A pandas dataframe\n",
    "                    column (series): A pandas series\n",
    "\n",
    "            Returns:\n",
    "                    word_list (list): A list of cleaned words\n",
    "    \"\"\"\n",
    "    return clean(''.join(str(df[column].tolist())))\n",
    "\n",
    "get_words(df, 'comment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csat =  df.loc[df['score'] == 1]\n",
    "df_dsat =  df.loc[df['score'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csat_words = get_words(df_csat, 'comment')\n",
    "dsat_words = get_words(df_dsat, 'comment')\n",
    "all_words = get_words(df, 'comment')\n",
    "\n",
    "csat_freq = pd.Series(csat_words).value_counts()\n",
    "dsat_freq = pd.Series(dsat_words).value_counts()\n",
    "all_freq = pd.Series(all_words).value_counts()\n",
    "\n",
    "word_counts = (pd.concat([all_freq, csat_freq, dsat_freq], axis=1, sort=True)\n",
    "                .set_axis(['all', 'csat', 'dsat'], axis=1, inplace=False)\n",
    "                .fillna(0)\n",
    "                .apply(lambda s: s.astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the most frequently occuring words?\n",
    "word_counts.sort_values(by='all', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there any words that uniquely identify a dsat or csat comment?\n",
    "pd.concat([word_counts[word_counts.dsat == 0].sort_values(by='csat').tail(10),\n",
    "           word_counts[word_counts.csat == 0].sort_values(by='dsat').tail(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigrams(words):\n",
    "    \"\"\"\n",
    "    Takes in a list of words and returns a series of\n",
    "    unigrams with value counts.\n",
    "    \"\"\"\n",
    "    return  pd.Series(words).value_counts()\n",
    "\n",
    "def get_bigrams(words):\n",
    "    \"\"\"\n",
    "    Takes in a list of words and returns a series of\n",
    "    bigrams with value counts.\n",
    "    \"\"\"\n",
    "    return (pd.Series(nltk.ngrams(words, 2)).value_counts())[:20]\n",
    "\n",
    "def get_trigrams(words):\n",
    "    \"\"\"\n",
    "    Takes in a list of words and returns a series of\n",
    "    trigrams with value counts.\n",
    "    \"\"\"\n",
    "    return (pd.Series(nltk.ngrams(words, 3)).value_counts())[:20]\n",
    "\n",
    "def get_qualgrams(words):\n",
    "    \"\"\"\n",
    "    Takes in a list of words and returns a series of\n",
    "    qualgrams with value counts.\n",
    "    \"\"\"\n",
    "    return (pd.Series(nltk.ngrams(words, 4)).value_counts())[:20]\n",
    "\n",
    "def get_ngrams(df,column):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe with column name and generates a\n",
    "    dataframe of unigrams, bigrams, trigrams, and qualgrams.\n",
    "    \"\"\"\n",
    "    return get_bigrams(get_words(df,column)).to_frame().reset_index().rename(columns={'index':'bigram','0':'count'}), \\\n",
    "           get_trigrams(get_words(df,column)).to_frame().reset_index().rename(columns={'index':'trigram','0':'count'}), \\\n",
    "           get_qualgrams(get_words(df,column)).to_frame().reset_index().rename(columns={'index':'qualgram','0':'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize N-grams with Bar Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_bigrams(df, column, title):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe, target column name, and specified title\n",
    "    for the bar chart visualization of bigrams.\n",
    "    \"\"\"\n",
    "    get_bigrams(get_words(df,column)).sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Bigram')\n",
    "    plt.xlabel('# Occurances')\n",
    "\n",
    "def viz_trigrams(df, column, title):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe, target column name, and specified title\n",
    "    for the bar chart visualization of trigrams.\n",
    "    \"\"\"\n",
    "    get_trigrams(get_words(df,column)).sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Trigram')\n",
    "    plt.xlabel('# Occurances')\n",
    "    \n",
    "def viz_qualgrams(df, column, title):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe, target column name, and specified title\n",
    "    for the bar chart visualization of qualgrams.\n",
    "    \"\"\"\n",
    "    get_bigrams(get_words(df,column)).sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Qualgram')\n",
    "    plt.xlabel('# Occurances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating DataFrames Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1,df2,df3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Dataframe Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df1.merge(df2,\n",
    "                      left_on='id1',\n",
    "                      right_on='id2',\n",
    "                      suffixes=('_left', '_right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using iLoc to Select Rows of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single selections using iloc and DataFrame\n",
    "\n",
    "# Rows:\n",
    "df.iloc[0] # first row of data frame (Aleshia Tomkiewicz) - Note a Series data type output.\n",
    "df.iloc[1] # second row of data frame (Evan Zigomalas)\n",
    "df.iloc[-1] # last row of data frame (Mi Richan)\n",
    "\n",
    "# Columns:\n",
    "df.iloc[:,0] # first column of data frame (first_name)\n",
    "df.iloc[:,1] # second column of data frame (last_name)\n",
    "df.iloc[:,-1] # last column of data frame (id)\n",
    "\n",
    "# Multiple row and column selections using iloc and DataFrame\n",
    "\n",
    "df.iloc[0:5] # first five rows of dataframe\n",
    "df.iloc[:, 0:2] # first two columns of data frame with all rows\n",
    "df.iloc[[0,3,6,24], [0,5,6]] # 1st, 4th, 7th, 25th row + 1st 6th 7th columns.\n",
    "df.iloc[0:5, 5:8] # first 5 rows and 5th, 6th, 7th columns of data frame (county -> phone1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using loc to Select Rows of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows with first name Ednalyn, include all columns between 'city' and 'email'\n",
    "df.loc[data['first_name'] == 'Ednalyn', 'city':'email']\n",
    " \n",
    "# Select rows where the email column ends with 'gmail.com', include all columns\n",
    "df.loc[data['email'].str.endswith(\"gmail.com\")]   \n",
    " \n",
    "# Select rows with first_name equal to some values, all columns\n",
    "df.loc[data['first_name'].isin(['Ednalyn', 'Ederlyne', 'Edelyn'])]   \n",
    "       \n",
    "# Select rows with first name Ednalyn and gmail email addresses\n",
    "df.loc[data['email'].str.endswith(\"gmail.com\") & (data['first_name'] == 'Ednalyn')] \n",
    " \n",
    "# select rows with id column between 100 and 200, and just return 'zip' and 'web' columns\n",
    "df.loc[(data['id'] > 100) & (df['id'] <= 200), ['zip', 'web']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying the First and Last Rows of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Shape of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying the First Ten Items of a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifying Column Names in Bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=['column_name', 'column_name', 'column_name', 'column_name', 'column_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handpicking Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = df[['column_name',\n",
    "          'column_name',\n",
    "          'column_name',\n",
    "          'column_name',\n",
    "          'column_name'\n",
    "        ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'old_name':'new_name',\n",
    "                        'old_name':'new_name',\n",
    "                        'old_name':'new_name',\n",
    "                        'old_name':'new_name',\n",
    "                        'old_name':'new_name'\n",
    "                        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset =\"column_id\", keep = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0','Unnamed: 0.1','score'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('date_time_zone', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resetting the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding DataFrame Column Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col_total'] = df.col1 + df.col2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Column Based on a Conditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'], \n",
    "        'age': [42, 52, 36, 24, 73], \n",
    "        'preTestScore': [4, 24, 31, 2, 3],\n",
    "        'postTestScore': [25, 94, 57, 62, 70]}\n",
    "df = pd.DataFrame(data, columns = ['name', 'age', 'preTestScore', 'postTestScore'])\n",
    "\n",
    "df['elderly'] = np.where(df['age']>=50, 'yes', 'no')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Column Based on Existing Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Date':['10/2/2011', '11/2/2011', '12/2/2011', '13/2/2011'], \n",
    "                    'Event':['Music', 'Poetry', 'Theatre', 'Comedy'], \n",
    "                    'Cost':[10000, 5000, 15000, 2000]})\n",
    "\n",
    "df['Discounted_Price'] = df.apply(lambda row: row.Cost - \n",
    "                                  (row.Cost * 0.1), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Non-null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df['column_name'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Rows Where a Column is Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['col_name'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Rows Where Column is in List of Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['col_name'].isin(list_of_values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Rows Where Column is Not in List of Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['col_name'].isin(list_of_values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Rows Based on Multiple Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['column_name'] >= A) & (df['column_name'] <= B)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.col_name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Counts Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.concat([df.rating.value_counts(),\n",
    "                    df.rating.value_counts(normalize=True)], axis=1)\n",
    "labels.columns = ['n', 'percent']\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a List from Value Counts Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_index_values = df.col_name.value_counts(dropna=False).index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a DataFrame from Value Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.column_name.value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info and Describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "df[\"startDate\"] = pd.to_datetime(df[\"startDate\"])\n",
    "\n",
    "# extract time components from the timestamp column\n",
    "df2['year'] = df2.timestamp.dt.year\n",
    "df2['month'] = df2.timestamp.dt.month\n",
    "df2['day'] = df2.timestamp.dt.day\n",
    "df2['hour'] = df2.timestamp.dt.hour\n",
    "\n",
    "# extract time attributes from the timestamp column\n",
    "df2['day_of_year'] = df2.timestamp.dt.dayofyear\n",
    "df2['week_of_year'] = df2.timestamp.dt.weekofyear\n",
    "df2['day_of_week'] = df2.timestamp.dt.dayofweek\n",
    "\n",
    "# extract weekday characteristic from the day of the week column\n",
    "df2['weekday'] = np.where(df2['day_of_week'] < 5, True, False)\n",
    "\n",
    "# extract part of the day from the hour column\n",
    "def get_day_period(x):\n",
    "    if x in range(6,12):\n",
    "        return 'Morning'\n",
    "    elif x in range(12,18):\n",
    "        return 'Afternoon'\n",
    "    elif x in range(18,23):\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Late night'\n",
    "\n",
    "df2['part_of_day'] = df2['hour'].apply(get_day_period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designating CSAT vs DSAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a new column and designates a row as either high or low\n",
    "df['csat'] = np.where(df['rating']>=3, 'high', 'low')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting CSAT and DSAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positive =  df.loc[df['column_name'] == 'positive']\n",
    "df_negative =  df.loc[df['column_name'] == 'negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [10, 6]\n",
    "plt.rcParams[\"figure.autolayout\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the default Pearson for continuous variables\n",
    "corr_matrix = df_totals.corr(method ='pearson')\n",
    "\n",
    "# Use Spearman for ordinal variables\n",
    "corr_matrix = df_totals.corr(method ='spearman')\n",
    "\n",
    "# Setup\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "# vmin and vmax control the range of the colormap\n",
    "sns.heatmap(corr_matrix, cmap='RdBu', annot=True, fmt='.2f',\n",
    "           vmin=-1, vmax=1)\n",
    "\n",
    "plt.title(\"Correlations Between Something and Somethings\")\n",
    "\n",
    "# Add tight_layout to ensure the labels don't get cut off\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Violin Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "ax = sns.violinplot(x=\"category\", y=\"numeric\", data=df)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(x=\"category1\", y=\"numeric_measure\",\n",
    "                hue=\"binary_category\", col=\"category2\",\n",
    "                data=df, kind=\"violin\", split=True,\n",
    "                height=6, aspect=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Levene's Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.levene(df_group1['col_name'], df_group2['col_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Levene's Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levene_hom = []\n",
    "levene_het = []\n",
    "\n",
    "for column in columns_list:\n",
    "    \n",
    "    result = stats.levene(df_group1[column], df_group2[column])[1]\n",
    "    \n",
    "    if result > ALPHA:\n",
    "        interpretation = 'insignificant - HOMOGENOUS'\n",
    "        levene_hom.append(column)\n",
    "    else:\n",
    "        interpretation = 'significant - HETEROGENOUS'\n",
    "        levene_het.append(column)\n",
    "        \n",
    "    print(result, '-', column, ' - ', interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shapiro Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = df_group1['col_name'] - df_group2['col_name']\n",
    "stats.shapiro(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mann-Whitney U Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.05\n",
    "\n",
    "stat, p = mannwhitneyu(df_group1[column], df_group2[column], df_group3[column])\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    print('Same distribution (fail to reject H0)')\n",
    "else:\n",
    "    print('Different distribution (reject H0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mann-Whitney U Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.05\n",
    "\n",
    "mannwhitneyu_same = []\n",
    "mannwhitneyu_diff = []\n",
    "\n",
    "for column in columns_list:\n",
    "    stat, p = mannwhitneyu(df_group1[column], df_group2[column], df_group3[column])\n",
    "    \n",
    "    if p > ALPHA:\n",
    "        interpretation = 'SAME (fail to reject H0)'\n",
    "        print('Statistics=%.3f, p=%.3f' % (stat, p) + ' - ' + column + ' - ' + interpretation)\n",
    "        mannwhitneyu_same.append(column)\n",
    "    else:\n",
    "        interpretation = 'DIFFERENT (reject H0)'\n",
    "        print('Statistics=%.3f, p=%.3f' % (stat, p) + ' - ' + column + ' - ' + interpretation)\n",
    "        mannwhitneyu_diff.append(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent T-testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(df_group1['col_name'], df_group2['col_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent T-testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.05\n",
    "\n",
    "for column in levene_hom:\n",
    "    \n",
    "    result = stats.ttest_ind(df_group1[column], df_group2[column])[1]\n",
    "    \n",
    "    if result > ALPHA:\n",
    "        interpretation = 'insignificant - SAME'\n",
    "        ttest_same.append(column)\n",
    "    else:\n",
    "        interpretation = 'significant - DIFFERENT'\n",
    "        ttest_diff.append(column)\n",
    "        \n",
    "    print(result, '-', column, ' - ', interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Treemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.treemap(df, path=['case_origin', 'case_tag'], values='score')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale continuous variables\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(df[['id_age']])\n",
    "\n",
    "df[['id_age']] = scaler.transform(df[['id_age']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_values(df4, ['high'])\n",
    "\n",
    "X = df4.loc[:, df4.columns != 'high']\n",
    "y = df4.loc[:, df4.columns == 'high']\n",
    "\n",
    "os = SMOTE(random_state=493)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=493)\n",
    "\n",
    "columns = X_train.columns\n",
    "os_data_X,os_data_y=os.fit_resample(X_train, y_train)\n",
    "os_data_X = pd.DataFrame(data=os_data_X,columns=columns )\n",
    "os_data_y= pd.DataFrame(data=os_data_y,columns=['high'])\n",
    "\n",
    "# we can check the numbers of our data\n",
    "print(\"length of oversampled data is \",len(os_data_X))\n",
    "print(\"Number of no subscription in oversampled data\",len(os_data_y[os_data_y['high']==0]))\n",
    "print(\"Number of subscription\",len(os_data_y[os_data_y['high']==1]))\n",
    "print(\"Proportion of no subscription data in oversampled data is \",len(os_data_y[os_data_y['high']==0])/len(os_data_X))\n",
    "print(\"Proportion of subscription data in oversampled data is \",len(os_data_y[os_data_y['high']==1])/len(os_data_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Number of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of features\n",
    "features = [col for col in df]\n",
    "\n",
    "# create list of lists that increase number of features in list\n",
    "that_list = []\n",
    "list_of_feature_lists = []\n",
    "for i in range(1,len(features)):\n",
    "    that_list = features[0:i]\n",
    "    list_of_feature_lists.append(that_list)\n",
    "    \n",
    "# loop to identify efficacy of models\n",
    "for list_item in list_of_feature_lists:\n",
    "    xgb_cl = xgb.XGBClassifier()\n",
    "    xgb_cl.fit(os_data_X[list_item], os_data_y)\n",
    "    preds = xgb_cl.predict(X_test[list_item])\n",
    "\n",
    "    print(\"Accuracy Score: \" + str(accuracy_score(y_test, preds)))\n",
    "    print(\"Number of features: \" + str(len(list_item)))\n",
    "    print('--------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=493)\n",
    "\n",
    "logreg = LogisticRegression(solver=\"saga\")\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(confusion_matrix)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-Series Forecasting with Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.read_csv('blood_glucose.csv', parse_dates=True, infer_datetime_format=True)\n",
    "df5 = df5.rename(columns={'startDate':'ds', 'value':'y'})\n",
    "df5[\"ds\"] = pd.to_datetime(df5[\"ds\"])\n",
    "df5['ds'] = df5['ds'].dt.tz_localize(None)\n",
    "df6=df5.set_index('ds').resample('D').agg(y=('y', 'mean'))\n",
    "df6 = df6.reset_index()\n",
    "\n",
    "model = Prophet()\n",
    "model.fit(df6)\n",
    "df6_forecast = model.make_future_dataframe(periods=12, freq='MS')\n",
    "df6_forecast = model.predict(df6_forecast)\n",
    "plt.figure(figsize=(18, 6))\n",
    "model.plot(df6_forecast, xlabel = 'Timestamp', ylabel = 'Glucose')\n",
    "plt.title('Blood Glucose')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XML to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create element tree object\n",
    "tree = ET.parse('export.xml')\n",
    "\n",
    "# extract the attributes for every health record\n",
    "root = tree.getroot()\n",
    "record_list = [x.attrib for x in root.iter('Record')]\n",
    "\n",
    "# create the dataframe\n",
    "df = pd.DataFrame(record_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
